import { GoogleGenerativeAI } from '@google/generative-ai';
import { generateEmbedding } from './embeddings';
import { searchSimilarChunks, getDocumentStats, searchByKeyword } from './vectorDB';
import { analyzeQuery, generateAggregationPrompt, QueryAnalysis } from './queryAnalyzer';
import { multiQuerySearch, groupChunksByDocument, formatGroupedContext } from './multiQueryRAG';
import { rerankChunks } from './reranker';
import * as sessionMemory from './sessionMemory';
import * as cacheService from './cacheService';
import Groq from 'groq-sdk';
import notificationService from '../notificationService';
import { prisma } from '../../config/database';
import * as path from 'path';
import * as fs from 'fs';

function logDebug(msg: string) {
    try {
        fs.appendFileSync(path.join(process.cwd(), 'debug_log.txt'), `[${new Date().toISOString()}] ${msg}\n`);
    } catch (e) {
        // console.error('Failed to write to debug log', e);
    }
}


// Gemini 2.5 Flash como provider principal, Groq como fallback
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
const groq = new Groq({ apiKey: process.env.GROQ_API_KEY || 'dummy' });

// Modelo principal: Gemini 2.5 Flash
const GEMINI_MODEL = 'gemini-2.5-flash';
const GROQ_MODEL = 'llama-3.3-70b-versatile';

export interface ChatMessage {
    role: 'user' | 'assistant';
    content: string;
}

export interface ChatResponse {
    response: string;
    sources: Array<{
        fileName: string;
        chunkIndex: number;
        similarity: number;
    }>;
    tokenUsage?: {
        inputTokens: number;
        outputTokens: number;
        totalTokens: number;
        model: string;
    };
    fromCache?: boolean;  // Indica se a resposta veio do cache
}

// Log startup to confirm code is fresh
logDebug('ChatService module loaded - Code Updated');


export interface UserProfile {
    userId?: string;  // Required for session context
    name?: string;
    jobTitle?: string;
    department?: string;
    technicalLevel?: string;
    communicationStyle?: string;
}

/**
 * Generate a greeting response without RAG (for simple greetings)
 */
function generateGreetingResponse(question: string, mode: string): string {
    const lowerQ = question.toLowerCase().trim();

    // Detect time-based greetings
    const hour = new Date().getHours();
    let timeGreeting = 'OlÃ¡';
    if (hour >= 5 && hour < 12) timeGreeting = 'Bom dia';
    else if (hour >= 12 && hour < 18) timeGreeting = 'Boa tarde';
    else timeGreeting = 'Boa noite';

    // Professional mode greeting
    if (mode === 'professional') {
        return `${timeGreeting}! Sou o assistente comercial da Tecfag. Como posso ajudÃ¡-lo hoje com nossas soluÃ§Ãµes de equipamentos e automaÃ§Ã£o?`;
    }

    // Casual mode greeting
    if (mode === 'casual') {
        return `${timeGreeting}! ğŸ‘‹ Tudo bem? Estou aqui para ajudar com qualquer dÃºvida sobre os produtos e soluÃ§Ãµes da Tecfag. O que precisa?`;
    }

    // Direct mode greeting
    if (mode === 'direct') {
        return `${timeGreeting}. Como posso ajudar?`;
    }

    // Educational/default greeting
    return `${timeGreeting}! Sou o assistente tÃ©cnico da Tecfag. Estou aqui para ajudar com informaÃ§Ãµes sobre nossos equipamentos, especificaÃ§Ãµes tÃ©cnicas e orientaÃ§Ãµes. Como posso ajudÃ¡-lo hoje?`;
}

/**
 * Answer a question using RAG (Retrieval Augmented Generation)
 */
export async function answerQuestion(
    question: string,
    catalogId?: string,
    chatHistory: ChatMessage[] = [],
    mode: 'direct' | 'casual' | 'educational' | 'professional' = 'educational',
    isTableMode: boolean = false,
    userProfile?: UserProfile,
    isAttachmentMode: boolean = false
): Promise<ChatResponse> {
    try {
        console.log(`[ChatService] Processing question: ${question.substring(0, 50)}... (Mode: ${mode}, Provider: Gemini 2.5 Flash, Attachment: ${isAttachmentMode})`);
        console.log(`[ChatService] DEBUG: catalogId=${catalogId}, userProfile=${userProfile ? userProfile.userId : 'none'}`);
        logDebug(`Processing question: "${question}"`);
        logDebug(`Params: catalogId=${catalogId}, mode=${mode}`);


        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // SESSION CONTEXT: Process user context for memory-aware responses
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let contextInstruction = '';
        if (userProfile?.userId) {
            try {
                const responseFormat = isTableMode ? 'table' : 'normal';
                const { contextInstruction: ctxInst } = await sessionMemory.processMessageContext(
                    userProfile.userId,
                    question,
                    responseFormat
                );
                contextInstruction = ctxInst;
                console.log(`[ChatService] Session context loaded for user ${userProfile.userId}`);
            } catch (ctxError: any) {
                console.warn(`[ChatService] Session context error (non-fatal): ${ctxError.message}`);
            }
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // ADVANCED RAG: Step 1 - Analyze the query to determine strategy
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        const queryAnalysis = analyzeQuery(question);

        console.log(`[ChatService] Query Analysis:`, {
            type: queryAnalysis.type,
            contextSize: queryAnalysis.contextSize,
            needsMultiQuery: queryAnalysis.needsMultiQuery,
            isCountQuery: queryAnalysis.isCountQuery,
            categories: queryAnalysis.categories
        });

        // Handle greetings without RAG
        if (queryAnalysis.type === 'greeting') {
            return {
                response: generateGreetingResponse(question, mode),
                sources: []
            };
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // CACHE: Check for existing identical queries
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        const cachedExact = await cacheService.getCachedResponseExact(question, catalogId, userProfile?.userId);
        if (cachedExact) {
            console.log(`[ChatService] ğŸš€ Returning CACHED response (exact match)`);
            logDebug(`CACHE HIT (Exact): ${cachedExact.id}`);
            await cacheService.recordCacheHit(cachedExact.id);
            return {
                response: cachedExact.response,
                sources: cachedExact.sources,
                fromCache: true
            };
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // ADVANCED RAG: Step 2 - Multi-query search or standard search
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let relevantChunks: any[] = [];
        let searchMetadata = '';

        if (queryAnalysis.needsMultiQuery) {
            // Use advanced multi-query RAG for aggregation/exploratory queries
            console.log(`[ChatService] Using Multi-Query RAG (${queryAnalysis.suggestedQueries.length + 1} queries)`);

            const multiResult = await multiQuerySearch(question, queryAnalysis, catalogId);
            relevantChunks = multiResult.chunks;

            // Add metadata about the search for better context
            searchMetadata = `
ğŸ“Š INFORMAÃ‡ÃƒO DO SISTEMA (use para contexto):
- Foram consultados ${multiResult.uniqueDocuments.length} documentos diferentes
- Recuperados ${multiResult.chunks.length} trechos relevantes
- Queries executadas: ${multiResult.queryBreakdown.map(q => q.query).slice(0, 3).join(', ')}...
`;

            // For count queries, also get document stats
            if (queryAnalysis.isCountQuery) {
                const stats = await getDocumentStats(catalogId);
                searchMetadata += `
ğŸ“ˆ ESTATÃSTICAS DA BASE:
- Total de documentos indexados: ${stats.totalDocuments}
- Total de chunks na base: ${stats.totalChunks}
- Documentos: ${stats.documentNames.slice(0, 10).join(', ')}${stats.documentNames.length > 10 ? '...' : ''}
`;
            }
        } else {
            // Standard semantic search for factual queries
            // First check embedding cache
            let questionEmbedding = await cacheService.getEmbeddingFromCache(question);
            if (!questionEmbedding) {
                questionEmbedding = await generateEmbedding(question);
                await cacheService.cacheEmbedding(question, questionEmbedding);
            }

            // Check semantic cache before doing full RAG
            // Perform parallel searches: Semantic + Keyword (Hybrid Search)
            // Re-enabled semantic cache
            const cachedSemantic = await cacheService.getCachedResponseSemantic(questionEmbedding, catalogId, userProfile?.userId);
            if (cachedSemantic) {
                console.log(`[ChatService] ğŸš€ Returning CACHED response (semantic match)`);
                logDebug(`CACHE HIT (Semantic): ${cachedSemantic.id}`);
                await cacheService.recordCacheHit(cachedSemantic.id);
                return {
                    response: cachedSemantic.response,
                    sources: cachedSemantic.sources,
                    fromCache: true
                };
            }

            // Perform parallel searches: Semantic + Keyword (Hybrid Search)
            console.log(`[ChatService] ğŸ” Starting Hybrid Search for: "${question}"`);

            // 1. Semantic Search (Massively increased retrieval for Gemini Context)
            const semanticChunks = await searchSimilarChunks(
                questionEmbedding,
                150, // ğŸš€ UPDATED: Increased to 150 to capture "Compilado" AND specific docs
                catalogId ? { catalogId } : undefined
            );

            // 2. Keyword Search
            const keywordChunks = await searchByKeyword(
                question,
                20, // Increased keyword matches
                catalogId ? { catalogId } : undefined
            );

            // Merge and Deduplicate
            const uniqueChunkIds = new Set<string>();
            relevantChunks = [];

            // 1. Add keyword chunks (High priority)
            for (const chunk of keywordChunks) {
                if (!uniqueChunkIds.has(chunk.id)) {
                    uniqueChunkIds.add(chunk.id);
                    relevantChunks.push(chunk);
                    console.log(`[ChatService] â• Added keyword match: ${chunk.metadata?.fileName || 'unknown'} (Sim: ${chunk.similarity})`);
                }
            }

            // 2. Add semantic chunks
            for (const chunk of semanticChunks) {
                if (!uniqueChunkIds.has(chunk.id)) {
                    uniqueChunkIds.add(chunk.id);
                    relevantChunks.push(chunk);
                }
            }

            // REVERTED: Removed "Diversity Check" to allow full reading of large documents
            // The AI is smart enough to handle 150+ chunks.

            console.log(`[ChatService] Hybrid Search Results: ${semanticChunks.length} semantic + ${keywordChunks.length} keyword = ${relevantChunks.length} unique chunks`);
        }

        console.log(`[ChatService] Found ${relevantChunks.length} relevant chunks`);
        logDebug(`Found ${relevantChunks.length} chunks.`);
        if (relevantChunks.length > 0) {
            console.log(`[ChatService] DEBUG: First chunk ID: ${relevantChunks[0].id}`);
            console.log(`[ChatService] DEBUG: First chunk content preview: ${relevantChunks[0].content.substring(0, 100)}...`);
            console.log(`[ChatService] DEBUG: First chunk metadata: ${JSON.stringify(relevantChunks[0].metadata)}`);

            logDebug(`First chunk: ${relevantChunks[0].id} - ${relevantChunks[0].metadata?.fileName}`);

            // Log ALL chunks for deep debugging
            relevantChunks.forEach((c, i) => {
                logDebug(`Chunk [${i}]: ${c.metadata?.fileName} (Sim: ${c.similarity}, ID: ${c.id})`);
            });

        } else {
            logDebug('NO CHUNKS FOUND - Search failed');
        }


        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // RERANKING: Use Gemini to reorder chunks by relevance
        // This is KEY for matching NotebookLM quality
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if (relevantChunks.length > 10 &&
            (queryAnalysis.type === 'recommendation' || queryAnalysis.type === 'exploratory' || queryAnalysis.type === 'comparative')) {
            console.log(`[ChatService] ğŸ¯ Applying reranking for ${queryAnalysis.type} query...`);
            try {
                const rerankedChunks = await rerankChunks(question, relevantChunks, Math.min(relevantChunks.length, 50));
                // Convert back to expected format
                relevantChunks = rerankedChunks.map(rc => ({
                    id: rc.id,
                    content: rc.content,
                    similarity: rc.combinedScore, // Use combined score
                    metadata: rc.metadata,
                    documentId: rc.documentId,
                    chunkIndex: rc.chunkIndex
                }));
                console.log(`[ChatService] âœ… Reranking complete. Top chunk score: ${relevantChunks[0]?.similarity.toFixed(3)}`);
            } catch (rerankError: any) {
                console.warn(`[ChatService] âš ï¸ Reranking failed, using original order: ${rerankError.message}`);
            }
        }

        // Log chunk distribution for debugging
        const chunksByDoc = new Map<string, number>();
        for (const chunk of relevantChunks) {
            const fileName = chunk.metadata?.fileName || 'Unknown';
            chunksByDoc.set(fileName, (chunksByDoc.get(fileName) || 0) + 1);
        }
        console.log(`[ChatService] Chunk distribution:`, Object.fromEntries(chunksByDoc));

        if (relevantChunks.length === 0) {
            return {
                response: 'NÃ£o encontrei informaÃ§Ãµes suficientes nos documentos para responder sua pergunta com a profundidade necessÃ¡ria. Tente adicionar mais documentos relacionados ou reformule a pergunta.',
                sources: []
            };
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // ADVANCED RAG: Step 3 - Build context (grouped by document for aggregation)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let context: string;

        if (queryAnalysis.type === 'aggregation' || queryAnalysis.type === 'exploratory') {
            // Group chunks by document for better understanding
            const groupedChunks = groupChunksByDocument(relevantChunks);
            context = formatGroupedContext(groupedChunks);
        } else {
            // Standard context formatting for factual queries
            context = relevantChunks
                .map((chunk, index) => {
                    const metadata = chunk.metadata || {};
                    return `[ID: ${index + 1} | Fonte: ${metadata.fileName || 'Documento'}]
${chunk.content}`;
                })
                .join('\n\n---\n\n');
        }

        // Add aggregation-specific prompt if needed
        const aggregationPrompt = generateAggregationPrompt(question, queryAnalysis);

        // Add recommendation-specific prompt for multi-option responses
        const recommendationPrompt = queryAnalysis.type === 'recommendation' ? `
ğŸ“ **PERGUNTA DE RECOMENDAÃ‡ÃƒO DETECTADA**
Esta pergunta pede uma RECOMENDAÃ‡ÃƒO de equipamento/mÃ¡quina. Siga estas regras OBRIGATÃ“RIAS:

1. **IDENTIFICAÃ‡ÃƒO DO PRODUTO** (SE NÃƒO ESPECIFICADO):
   - Se a pergunta nÃ£o especificar CLARAMENTE o tipo de produto (pÃ³, lÃ­quido, grÃ£o, pastoso), o formato desejado (sachÃª stick, sachÃª 3 soldas, pouch, etc.), ou o volume de produÃ§Ã£o, PERGUNTE PRIMEIRO antes de recomendar.
   - Exemplo: "Para recomendar a mÃ¡quina ideal, preciso saber: qual tipo de produto serÃ¡ embalado (pÃ³, grÃ£os, lÃ­quido)? Qual o formato de sachÃª desejado?"

2. **MÃšLTIPLAS OPÃ‡Ã•ES (OBRIGATÃ“RIO)**:
   - SEMPRE apresente pelo menos **3-4 opÃ§Ãµes de mÃ¡quinas diferentes**, organizadas por categoria/aplicaÃ§Ã£o.
   - Use formataÃ§Ã£o estruturada com headers para cada categoria:
     ### 1. Para SachÃªs tipo [Tipo] (AplicaÃ§Ã£o)
     - **Modelo**: [Nome da mÃ¡quina]
     - **Dosagem**: [Faixa em g ou ml]
     - **Produtividade**: [embalagens/min]
     - **Tipo de Selagem**: [descriÃ§Ã£o]
   
3. **COMPARAÃ‡ÃƒO PRÃTICA**:
   - Ao final, compare brevemente QUANDO cada opÃ§Ã£o Ã© mais indicada.
   - Adicione uma "Dica de Especialista" BREVE (mÃ¡ximo 2-3 linhas) - NÃƒO domine a resposta com metodologia SPICED.

4. **PRIORIZE ESPECIFICAÃ‡Ã•ES TÃ‰CNICAS**:
   - Foque em informaÃ§Ãµes objetivas: capacidade, produtividade, tipo de selagem.
   - NÃƒO use metodologia SPICED para perguntas de recomendaÃ§Ã£o tÃ©cnica.
   - Seja como um catÃ¡logo tÃ©cnico interativo, nÃ£o um vendedor.

EXEMPLO DE ESTRUTURA ESPERADA:
"Para recomendar a mÃ¡quina ideal para embalar e envasar sachÃª, identifiquei as seguintes opÃ§Ãµes com base no tipo de produto:

### 1. Para SachÃªs tipo Stick (PÃ³s)
- **Modelo**: AFPP2830B
- **Dosagem**: 10 a 30g
- **Produtividade**: 25 a 35 emb/min
- **Selagem**: 3 soldas

### 2. Para SachÃªs Tradicionais (PÃ³s/Granulados)
[...]

### Qual escolher?
- Se o produto for pÃ³ fino: opte pela AFPP2830B
- Se precisar de sachÃªs com 4 soldas: AFPP1528A
[...]"` : '';

        // Build User Profile Context
        let userProfileContext = '';
        if (userProfile) {
            userProfileContext = `
PERFIL DO USUÃRIO (Personalize a resposta para esta pessoa):
- Nome: ${userProfile.name || 'Desconhecido'}
- Cargo: ${userProfile.jobTitle || 'NÃ£o informado'}
- Departamento: ${userProfile.department || 'NÃ£o informado'}
- NÃ­vel TÃ©cnico: ${userProfile.technicalLevel || 'PadrÃ£o'}
- Estilo Preferido: ${userProfile.communicationStyle || 'PadrÃ£o'}

INSTRUÃ‡ÃƒO DE PERSONALIZAÃ‡ÃƒO:
- Adapte o vocabulÃ¡rio e a profundidade tÃ©cnica ao NÃ­vel TÃ©cnico do usuÃ¡rio.
- DÃª exemplos relevantes ao Cargo e Departamento do usuÃ¡rio.
- Se o estilo for "Visual", use muitas listas e tabelas.
- Se o estilo for "Direto", seja extremamente conciso.
- Responda como se estivesse falando diretamente para esta pessoa especÃ­fica.
`;
        }

        // 4. Build prompt based on Mode
        let systemPrompt = '';

        const baseContext = `
REGRAS DE FONTE (RAG) - LEIA COM ATENÃ‡ÃƒO:

ğŸ“Œ REGRA PRINCIPAL - USO EXCLUSIVO DOS DOCUMENTOS:
- Baseie sua resposta ESTRITAMENTE nos documentos fornecidos abaixo.
- NÃƒO invente informaÃ§Ãµes que nÃ£o estejam nos documentos.
- NÃƒO busque informaÃ§Ãµes na internet, web, ou qualquer fonte externa.
- NÃƒO use seu conhecimento prÃ©vio de treinamento para complementar respostas.
- Toda informaÃ§Ã£o na sua resposta DEVE vir dos documentos anexados abaixo.

ğŸ“Œ REGRA CRÃTICA - USE TODOS OS DOCUMENTOS:
- VocÃª tem acesso a TODOS os documentos relevantes para esta pergunta.
- NUNCA diga que informaÃ§Ãµes nÃ£o estavam nos "documentos iniciais" - esse termo nÃ£o existe.
- NUNCA invente limitaÃ§Ãµes sobre quais documentos vocÃª tem acesso.
- Se a informaÃ§Ã£o estÃ¡ em QUALQUER documento fornecido, vocÃª DEVE incluÃ­-la na resposta.
- Analise TODOS os trechos fornecidos antes de responder.

ğŸ“Œ REGRA DE TRANSPARÃŠNCIA - QUANDO INFORMAÃ‡ÃƒO NÃƒO EXISTE:
- Se apÃ³s analisar TODOS os documentos fornecidos vocÃª nÃ£o encontrar a informaÃ§Ã£o solicitada, diga claramente:
  "NÃ£o encontrei informaÃ§Ãµes sobre [tema] nos documentos cadastrados no sistema. Pode ser que esse conteÃºdo ainda nÃ£o tenha sido adicionado Ã  base de conhecimento."
- Seja especÃ­fico sobre O QUE nÃ£o foi encontrado, nÃ£o generalize.
- NUNCA use a desculpa de "documentos iniciais" ou "primeiros documentos".

ğŸ“Œ CITAÃ‡ÃƒO DE FONTES:
- NÃƒO cite as fontes no texto da resposta (ex: "Segundo documento X").
- As fontes serÃ£o apresentadas separadamente pela interface do sistema.

${userProfileContext}
${searchMetadata}
${aggregationPrompt}
${recommendationPrompt}
${contextInstruction}

DOCUMENTOS DE REFERÃŠNCIA (USE TODO O CONTEÃšDO ABAIXO):
${context}`;

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // MEDIA LINKING: Find relevant images, PDFs and YouTube links in chunks
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let mediaAppendix = '';

        // Only process media if attachment mode is active
        if (isAttachmentMode) {
            // 1. Process Images
            const imageChunks = relevantChunks.filter(c => c.metadata?.isImage === true);

            if (imageChunks.length > 0) {
                console.log(`[ChatService] ğŸ–¼ï¸ Found ${imageChunks.length} image chunks relevant to query`);

                // Deduplicate images by normalized filename (case-insensitive, URL-decoded)
                const uniqueImages = new Set<string>();
                const images: Array<{ fileName: string, storedName: string }> = [];

                for (const chunk of imageChunks) {
                    const fileName = chunk.metadata.fileName;
                    // Normalize filename for deduplication
                    const normalizedFileName = decodeURIComponent(fileName || '').toLowerCase().trim();

                    if (fileName && !uniqueImages.has(normalizedFileName)) {
                        uniqueImages.add(normalizedFileName);

                        let storedName = chunk.metadata.storedFileName;

                        // Fallback: If storedFileName is missing (old documents), fetch from DB
                        if (!storedName && chunk.documentId) {
                            try {
                                const doc = await prisma.document.findUnique({
                                    where: { id: chunk.documentId },
                                    select: { filePath: true }
                                });
                                if (doc?.filePath) {
                                    storedName = path.basename(doc.filePath);
                                }
                            } catch (err) {
                                console.warn(`[ChatService] Failed to resolve file path for doc ${chunk.documentId}`);
                            }
                        }

                        images.push({
                            fileName,
                            storedName: storedName || fileName // Fallback to original name if all else fails
                        });
                    }
                }

                if (images.length > 0) {
                    mediaAppendix = '\n\n---\n\n### Imagens Relacionadas:\n\n';
                    images.forEach(img => {
                        // Encode filename for URL
                        const encodedName = encodeURIComponent(img.storedName);
                        // Use absolute URL to backend static files
                        // Assuming uploadDir is mounted at /uploads
                        const imageUrl = `http://localhost:3001/uploads/${encodedName}`;
                        mediaAppendix += `![${img.fileName}](${imageUrl})\n\n`;
                    });
                    console.log(`[ChatService] Added ${images.length} images to response`);
                }
            }

            // 2. Process PDFs (deliveryMode = 'media')
            const pdfChunks = relevantChunks.filter(c =>
                c.metadata?.isMedia === true && c.metadata?.isPDF === true
            );

            if (pdfChunks.length > 0) {
                console.log(`[ChatService] ğŸ“„ Found ${pdfChunks.length} PDF media chunks relevant to query`);

                const uniquePdfs = new Set<string>();
                const pdfs: Array<{ fileName: string, storedName: string }> = [];

                for (const chunk of pdfChunks) {
                    const fileName = chunk.metadata.fileName;
                    const normalizedFileName = decodeURIComponent(fileName || '').toLowerCase().trim();

                    if (fileName && !uniquePdfs.has(normalizedFileName)) {
                        uniquePdfs.add(normalizedFileName);

                        let storedName = chunk.metadata.storedFileName;

                        if (!storedName && chunk.documentId) {
                            try {
                                const doc = await prisma.document.findUnique({
                                    where: { id: chunk.documentId },
                                    select: { filePath: true }
                                });
                                if (doc?.filePath) {
                                    storedName = path.basename(doc.filePath);
                                }
                            } catch (err) {
                                console.warn(`[ChatService] Failed to resolve file path for PDF doc ${chunk.documentId}`);
                            }
                        }

                        pdfs.push({
                            fileName,
                            storedName: storedName || fileName
                        });
                    }
                }

                if (pdfs.length > 0) {
                    mediaAppendix += '\n\n### PDFs Relacionados:\n\n';
                    pdfs.forEach(pdf => {
                        const encodedName = encodeURIComponent(pdf.storedName);
                        const pdfUrl = `http://localhost:3001/uploads/${encodedName}`;
                        // Use special markdown syntax for PDFs: [pdf:filename](url)
                        mediaAppendix += `[pdf:${pdf.fileName}](${pdfUrl})\n\n`;
                    });
                    console.log(`[ChatService] Added ${pdfs.length} PDFs to response`);
                }
            }

            // 3. Detect YouTube Links in content
            const youtubeRegex = /(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:watch\?v=|embed\/)|youtu\.be\/)([a-zA-Z0-9_-]{11})(?:[&?].*)?/gi;
            const foundYouTubeIds = new Set<string>();

            for (const chunk of relevantChunks) {
                const content = chunk.content || '';
                let match;
                youtubeRegex.lastIndex = 0;

                while ((match = youtubeRegex.exec(content)) !== null) {
                    const videoId = match[1];
                    if (videoId && !foundYouTubeIds.has(videoId)) {
                        foundYouTubeIds.add(videoId);
                    }
                }
            }

            if (foundYouTubeIds.size > 0) {
                console.log(`[ChatService] ğŸ¬ Found ${foundYouTubeIds.size} YouTube videos in chunks`);
                mediaAppendix += '\n\n### VÃ­deos Relacionados:\n\n';
                foundYouTubeIds.forEach(videoId => {
                    const youtubeUrl = `https://www.youtube.com/watch?v=${videoId}`;
                    // Use special markdown syntax for YouTube: [youtube:videoId](url)
                    mediaAppendix += `[youtube:${videoId}](${youtubeUrl})\n\n`;
                });
                console.log(`[ChatService] Added ${foundYouTubeIds.size} YouTube links to response`);
            }
        }


        const tableInstruction = isTableMode
            ? `\n\nREQUISITO ESPECIAL DE FORMATAÃ‡ÃƒO:
- O usuÃ¡rio ATIVOU o "Modo Tabela".
- VocÃª DEVE apresentar a resposta ou parte significativa dela em formato de TABELA MARKDOWN sempre que houver dados comparÃ¡veis ou listÃ¡veis.
- Se a pergunta for sobre comparaÃ§Ã£o, diferenÃ§as, especificaÃ§Ãµes ou listas, a tabela Ã© OBRIGATÃ“RIA.
- Use colunas claras e objetivas.`
            : `\n\nREQUISITO DE FORMATAÃ‡ÃƒO:
- PREFIRA SEMPRE o formato de texto corrido ou listas com marcadores (â€¢).
- EVITE usar tabelas Markdown, a menos que o usuÃ¡rio peÃ§a explicitamente por "tabela" ou "comparativo em colunas".
- Para comparaÃ§Ãµes simples, use listas ou parÃ¡grafos contrastantes.`;

        switch (mode) {
            case 'direct':
                systemPrompt = `VocÃª Ã© um especialista tÃ©cnico da Tecfag que valoriza o tempo do colega.

Responda de forma objetiva e eficiente. Se for sim ou nÃ£o, comece assim.
Quando listar informaÃ§Ãµes, faÃ§a de forma organizada, mas sem perder naturalidade.
NÃ£o use introduÃ§Ãµes desnecessÃ¡rias - vÃ¡ direto ao que importa.

${baseContext}
${tableInstruction}`;
                break;

            case 'casual':
                systemPrompt = `CONTEXTO: VocÃª Ã© o LM (Learning Machine), um especialista tÃ©cnico sÃªnior da Tecfag.

MODO DE OPERAÃ‡ÃƒO: "CONSULTOR TÃ‰CNICO PROATIVO"

ğŸ“ **PROCESSO DE ATENDIMENTO OBRIGATÃ“RIO**:
Ao atender qualquer solicitaÃ§Ã£o de recomendaÃ§Ã£o ou dÃºvida sobre mÃ¡quinas, vocÃª DEVE seguir este fluxo:

1. **ANÃLISE DE NECESSIDADE (O MAIS IMPORTANTE)**:
   - Se o usuÃ¡rio pedir uma mÃ¡quina (ex: "preciso de uma envasadora"), vocÃª **NÃƒO PODE** simplesmente dar uma opÃ§Ã£o e encerrar.
   - VocÃª **DEVE** fazer o "DiagnÃ³stico 3D" (DimensÃ£o, Dinheiro, Desempenho) se essas informaÃ§Ãµes nÃ£o forem claras:
     a) **Qual o tamanho/dimensÃµes do produto?** (tamanho do sache, peso em gramas, etc)
     b) **Qual a velocidade de produÃ§Ã£o desejada?** (sachÃªs por minuto, garrafas por hora)
     c) **Qual a estimativa de investimento?** (para saber se sugerimos linha de entrada ou alta performance)

2. **REGRA DA MULTIPLICIDADE (SEMPRE MAIS DE UM)**:
   - Mesmo que o usuÃ¡rio peÃ§a UM modelo especÃ­fico, ou uma recomendaÃ§Ã£o genÃ©rica, vocÃª **SEMPRE DEVE SUGERIR 2 A 3 OPÃ‡Ã•ES**.
   - **Nunca** apresente apenas uma mÃ¡quina como soluÃ§Ã£o Ãºnica (salvo se for uma peÃ§a de reposiÃ§Ã£o especÃ­fica).
   - Apresente as opÃ§Ãµes como "SoluÃ§Ã£o Principal", "Alternativa de Maior Performance" e "Alternativa Custo-BenefÃ­cio".

3. **ESTRUTURA DA RESPOSTA**:
   - **IntroduÃ§Ã£o**: Confirme o entendimento da necessidade.
   - **Perguntas de DiagnÃ³stico**: (Se faltarem dados de tamanho, velocidade ou investimento).
   - **SugestÃµes de MÃ¡quinas**:
     - ğŸ”¹ **OpÃ§Ã£o A (Modelo X)**: Pontos fortes tÃ©cnicos.
     - ğŸ”¹ **OpÃ§Ã£o B (Modelo Y)**: Diferencial (mais rÃ¡pida ou mais econÃ´mica).
     - ğŸ”¹ **OpÃ§Ã£o C (Modelo Z)**: Outra alternativa relevante.
   - **Comparativo RÃ¡pido**: "A Modelo X Ã© melhor se vocÃª prioriza precisÃ£o, jÃ¡ a Modelo Y entrega mais velocidade..."

4. **TOM DE VOZ**:
   - TÃ©cnico, porÃ©m acessÃ­vel.
   - Investigativo (faÃ§a perguntas!).
   - Proativo (antecipe necessidades).

${baseContext}
${tableInstruction}`;
                break;

            case 'professional':
                systemPrompt = `CONTEXTO: VocÃª Ã© um CONSULTOR DE VENDAS ESPECIALISTA da Tecfag Group.

PAPEL E IDENTIDADE:
- VocÃª Ã© um especialista comercial da Tecfag Group com profundo conhecimento em soluÃ§Ãµes tÃ©cnicas, processos industriais e automaÃ§Ã£o.
- VocÃª fala como um consultor experiente conversando com um colega, NÃƒO como um robÃ´ ou chatbot.
- Seu objetivo Ã© ENSINAR o vendedor a vender de forma consultiva, nÃ£o apenas listar informaÃ§Ãµes.

DETECÃ‡ÃƒO DE CONTEXTO E PROPORÃ‡ÃƒO DE RESPOSTA (CRÃTICO):
Antes de responder, AVALIE a complexidade e o tipo da pergunta:

ğŸ“ **SAUDAÃ‡Ã•ES E MENSAGENS SOCIAIS** (ex: "bom dia", "olÃ¡", "como vai?"):
- Responda de forma CORDIAL e BREVE
- NÃƒO aplique SPICED
- NÃƒO inclua Dica de Especialista
- NÃƒO liste produtos ou soluÃ§Ãµes nÃ£o solicitados
- Exemplo: "Bom dia! Como posso ajudÃ¡-lo hoje com as soluÃ§Ãµes da Tecfag?"

ğŸ“ **PERGUNTAS FACTUAIS SIMPLES** (ex: "Qual o preÃ§o?", "Onde fica a empresa?"):
- Responda DIRETAMENTE com a informaÃ§Ã£o solicitada
- NÃƒO aplique SPICED
- Seja objetivo e profissional

ğŸ“ **PERGUNTAS SOBRE VENDAS/CONSULTORIA** (ex: "Como vender X?", "Como usar tÃ©cnica Y?"):
- APLIQUE SPICED de forma narrativa e fluida
- INCLUA Dica de Especialista com analogia memorÃ¡vel
- Use estrutura consultiva completa

ğŸ“ **PERGUNTAS TÃ‰CNICAS COMPLEXAS** (ex: "Como funciona X?", "Comparar A vs B"):
- Use abordagem consultiva com dados tÃ©cnicos integrados
- SPICED pode ser aplicado se agregar valor ao argumento de vendas
- Dica de Especialista OPCIONAL, apenas se genuinamente Ãºtil

METODOLOGIA DE VENDAS (SPICED - Uso Condicional):
Quando a pergunta for sobre VENDAS, CONSULTORIA ou PRODUTOS, estruture a resposta usando SPICED de forma NARRATIVA e FLUIDA:
- Situation: Explique como entender o contexto do cliente
- Pain: Identifique as dores especÃ­ficas que o produto resolve
- Impact: Quantifique o valor e ROI da soluÃ§Ã£o
- Critical Event: Identifique gatilhos de urgÃªncia
- Decision: Facilite o processo de decisÃ£o

ESTILO DE RESPOSTA NARRATIVO:
âœ… **FAÃ‡A:**
- Escreva como um especialista explicando para outro profissional (narrativa fluida, nÃ£o listas mecÃ¢nicas)
- Para cada etapa do SPICED, inclua uma **"Pergunta chave:"** especÃ­fica e prÃ¡tica que o vendedor pode usar
- Integre dados tÃ©cnicos NATURALMENTE no argumento de vendas (nÃ£o como lista separada)
- Use marcadores (â€¢) apenas para destacar pontos-chave dentro da narrativa
- Quando usar SPICED completo, inclua uma seÃ§Ã£o **"Dica de Especialista:"** com uma ANALOGIA MEMORÃVEL

âŒ **EVITE:**
- Aplicar estruturas complexas em perguntas simples
- Listas genÃ©ricas sem contexto
- Tom robÃ³tico ou formato de checklist
- Separar "BenefÃ­cios" do texto principal (integre no argumento)
- Perguntas vagas - seja ESPECÃFICO com dados do produto

ESTRUTURA ESPERADA (para perguntas de vendas/consultoria):
1. **IntroduÃ§Ã£o consultiva** explicando a abordagem
2. **Desenvolvimento narrativo** para cada etapa do SPICED:
   - ExplicaÃ§Ã£o do objetivo da etapa
   - â€¢ **AplicaÃ§Ã£o**: Como aplicar com o produto especÃ­fico
   - â€¢ **Pergunta chave**: "[pergunta especÃ­fica que o vendedor pode fazer]"
   - Destaque dados tÃ©cnicos integrados naturalmente
3. **Dica de Especialista**: Inclua analogia poderosa e memorÃ¡vel que compare o produto/processo atual a algo familiar
4. **ConclusÃ£o persuasiva** (opcional, se fizer sentido)

EXEMPLO DE TOM NARRATIVO:
âœ… "**1. SituaÃ§Ã£o (Situation)** - O objetivo aqui Ã© entender o contexto atual do cliente. Pergunte sobre o volume de produÃ§Ã£o e os materiais utilizados. â€¢ **AplicaÃ§Ã£o**: Verifique se o cliente trabalha com embalagens flexÃ­veis como PP, PE, BOPP. â€¢ **Pergunta chave**: 'Como Ã© o seu processo de selagem hoje e qual o tamanho da sua produÃ§Ã£o atual?'. Saiba que a TC20 Ã© ideal para pequena escala, mas com operaÃ§Ã£o contÃ­nua."

EXEMPLO DE ANALOGIA MEMORÃVEL:
âœ… "**Dica de Especialista:** Para facilitar o entendimento do cliente sobre a versatilidade da mÃ¡quina, use esta analogia: 'Imagine que sua produÃ§Ã£o hoje Ã© como lavar louÃ§a Ã  mÃ£o; vocÃª gasta tempo e esforÃ§o em cada peÃ§a individualmente. A Pratic Seal TC20 funciona como uma lavadora de louÃ§as: vocÃª apenas posiciona as embalagens na entrada e ela faz o trabalho de forma contÃ­nua e padronizada, permitindo que vocÃª foque em expandir seu negÃ³cio enquanto ela garante o fechamento perfeito.'"

INTEGRAÃ‡ÃƒO DE DADOS TÃ‰CNICOS:
- NÃƒO crie listas separadas de especificaÃ§Ãµes (exceto se solicitado ou em modo tabela)
- INTEGRE os dados tÃ©cnicos nos argumentos de forma natural
- Use os dados para QUANTIFICAR impacto e ROI

${baseContext}
${tableInstruction}

LEMBRE-SE: Seja PROPORCIONAL Ã  pergunta. SaudaÃ§Ãµes merecem saudaÃ§Ãµes. Perguntas complexas merecem respostas completas. Sua resposta deve parecer que foi escrita por um consultor HUMANO experiente que adapta sua comunicaÃ§Ã£o ao contexto.`;
                break;
            default:
                systemPrompt = `VocÃª Ã© um especialista tÃ©cnico da Tecfag explicando para um colega.

Sua paixÃ£o Ã© ensinar e fazer as pessoas entenderem de verdade.
Explique o raciocÃ­nio por trÃ¡s das coisas, nÃ£o apenas os fatos.
Use analogias quando ajudarem a clarear conceitos complexos.
Antecipe perguntas que a pessoa possa ter e responda-as naturalmente.

${baseContext}
${tableInstruction}`;
                break;
        }

        const userPrompt = `PERGUNTA DO USUÃRIO: "${question}"

Elabore uma resposta completa baseada nos documentos acima.`;

        // 5. Generate response - Gemini 2.5 Flash primary, Groq fallback
        let response: string = "";
        let tokenUsage: ChatResponse['tokenUsage'] = undefined;
        let usedFallback = false;

        // Build messages for Gemini
        const geminiMessages = [
            { role: 'user', parts: [{ text: systemPrompt }] },
            { role: 'model', parts: [{ text: `Entendido. Modo ${mode} ativado.` }] },
            ...chatHistory.slice(-6).map(msg => ({
                role: msg.role === 'user' ? 'user' : 'model',
                parts: [{ text: msg.content }]
            })),
            { role: 'user', parts: [{ text: userPrompt }] }
        ];

        try {
            // PRIMARY: Gemini 2.5 Pro (User Requested)
            const model = genAI.getGenerativeModel({ model: 'gemini-2.5-pro' });

            console.log(`[ChatService] Requesting completion from Gemini 2.5 Pro (High Reasoning)...`);
            const result = await model.generateContent({
                contents: geminiMessages as any,
                generationConfig: {
                    temperature: 0.2, // Low temp for analytical precision (NotebookLM style)
                    maxOutputTokens: 65536 // Massive output limit for full lists
                }
            });

            response = result.response.text();

            // Append media (images, PDFs, YouTube) if found
            if (mediaAppendix) {
                response += mediaAppendix;
            }

            // Capture token usage from Gemini
            const usageMetadata = result.response.usageMetadata;
            if (usageMetadata) {
                tokenUsage = {
                    inputTokens: usageMetadata.promptTokenCount || 0,
                    outputTokens: usageMetadata.candidatesTokenCount || 0,
                    totalTokens: usageMetadata.totalTokenCount || 0,
                    model: GEMINI_MODEL,
                };
                console.log(`[ChatService] âœ… Gemini 2.5 Flash - Token usage: ${tokenUsage.totalTokens} total (${tokenUsage.inputTokens} in, ${tokenUsage.outputTokens} out)`);
            }

        } catch (geminiError: any) {
            // FALLBACK: Groq (Llama 3.3 70B)
            console.warn(`[ChatService] âš ï¸ Gemini error: ${geminiError.message}. Switching to Groq fallback...`);
            usedFallback = true;

            try {
                const groqMessages: any[] = [
                    { role: 'system', content: systemPrompt },
                    ...chatHistory.slice(-4).map(msg => ({
                        role: msg.role === 'assistant' ? 'assistant' : 'user',
                        content: msg.content
                    })),
                    { role: 'user', content: userPrompt }
                ];

                console.log('[ChatService] Requesting completion from Groq (Llama 3.3 70B) as fallback...');

                const completion = await groq.chat.completions.create({
                    messages: groqMessages,
                    model: GROQ_MODEL,
                    temperature: 0.3,
                    max_tokens: 4096,
                    top_p: 0.9,
                });

                response = completion.choices[0]?.message?.content || "";
                response += '\n\n*(Backup: Groq Llama 3.3)*';

                // Append media (images, PDFs, YouTube) if found
                if (mediaAppendix) {
                    response += mediaAppendix;
                }

                // Capture token usage from Groq
                if (completion.usage) {
                    tokenUsage = {
                        inputTokens: completion.usage.prompt_tokens || 0,
                        outputTokens: completion.usage.completion_tokens || 0,
                        totalTokens: completion.usage.total_tokens || 0,
                        model: GROQ_MODEL + ' (fallback)',
                    };
                    console.log(`[ChatService] âœ… Groq Fallback - Token usage: ${tokenUsage.totalTokens} total`);
                }

            } catch (groqError: any) {
                console.error('[ChatService] âŒ Both Gemini and Groq failed:', groqError);

                // Notificar admins sobre falha crÃ­tica de IA
                await notificationService.broadcastToAdmins(
                    'system',
                    'ğŸš¨ Falha CrÃ­tica: APIs de IA IndisponÃ­veis',
                    `Gemini: ${geminiError.message} | Groq: ${groqError.message}`,
                    'error',
                    { geminiError: geminiError.message, groqError: groqError.message }
                );

                throw new Error(`AI providers unavailable: Gemini (${geminiError.message}), Groq (${groqError.message})`);
            }
        }

        console.log(`[ChatService] âœ… Generated response (${response.length} chars)`);

        // 6. Extract sources
        const sources = relevantChunks.map((chunk, index) => ({
            fileName: chunk.metadata?.fileName || 'Documento desconhecido',
            chunkIndex: chunk.chunkIndex,
            similarity: chunk.similarity
        }));

        // 7. Cache the response for future use
        try {
            const documentIds = [...new Set(relevantChunks.map(c => c.documentId).filter(Boolean))];
            const questionEmbedding = await cacheService.getEmbeddingFromCache(question) || await generateEmbedding(question);
            await cacheService.cacheResponse(
                question,
                questionEmbedding,
                response,
                sources,
                documentIds,
                catalogId,
                userProfile?.userId // Pass userId to scope the cache
            );
        } catch (cacheError: any) {
            console.warn(`[ChatService] Failed to cache response (non-fatal): ${cacheError.message}`);
        }

        return {
            response,
            sources,
            tokenUsage,
        };

    } catch (error: any) {
        console.error('[ChatService] Error:', error);
        throw new Error(`Failed to generate response: ${error.message}`);
    }
}

/**
 * Streaming version of answerQuestion - yields text chunks as they arrive
 */
export async function* answerQuestionStream(
    question: string,
    catalogId?: string,
    chatHistory: ChatMessage[] = [],
    mode: 'direct' | 'casual' | 'educational' | 'professional' = 'educational',
    isTableMode: boolean = false,
    userProfile?: UserProfile,
    isAttachmentMode: boolean = false
): AsyncGenerator<{ type: 'chunk' | 'done' | 'error'; content?: string; sources?: any[]; tokenUsage?: ChatResponse['tokenUsage'] }> {
    try {
        console.log(`[ChatService Stream] Processing question: ${question.substring(0, 50)}...`);

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // SESSION CONTEXT: Process user context for memory-aware responses
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let contextInstruction = '';
        if (userProfile?.userId) {
            try {
                const responseFormat = isTableMode ? 'table' : 'normal';
                const { contextInstruction: ctxInst } = await sessionMemory.processMessageContext(
                    userProfile.userId,
                    question,
                    responseFormat
                );
                contextInstruction = ctxInst;
                console.log(`[ChatService Stream] Session context loaded for user ${userProfile.userId}`);
            } catch (ctxError: any) {
                console.warn(`[ChatService Stream] Session context error (non-fatal): ${ctxError.message}`);
            }
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // STEP 1: Analyze query (same as non-streaming)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        const queryAnalysis = analyzeQuery(question);

        // Handle greetings without RAG
        if (queryAnalysis.type === 'greeting') {
            const greetingResponse = generateGreetingResponse(question, mode);
            yield { type: 'chunk', content: greetingResponse };
            yield { type: 'done', sources: [] };
            return;
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // STEP 2: Retrieve relevant chunks (same as non-streaming)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let relevantChunks;
        let searchMetadata = '';

        if (queryAnalysis.needsMultiQuery) {
            const multiResult = await multiQuerySearch(question, queryAnalysis, catalogId);
            relevantChunks = multiResult.chunks;
            searchMetadata = `\nğŸ“Š INFORMAÃ‡ÃƒO DO SISTEMA: Consultados ${multiResult.uniqueDocuments.length} documentos, ${multiResult.chunks.length} trechos.\n`;

            if (queryAnalysis.isCountQuery) {
                const stats = await getDocumentStats(catalogId);
                searchMetadata += `ğŸ“ˆ ESTATÃSTICAS: ${stats.totalDocuments} docs, ${stats.totalChunks} chunks.\n`;
            }
        } else {
            const questionEmbedding = await generateEmbedding(question);
            relevantChunks = await searchSimilarChunks(
                questionEmbedding,
                queryAnalysis.contextSize,
                catalogId ? { catalogId } : undefined
            );
        }

        if (relevantChunks.length === 0) {
            yield { type: 'chunk', content: 'NÃ£o encontrei informaÃ§Ãµes suficientes nos documentos para responder sua pergunta.' };
            yield { type: 'done', sources: [] };
            return;
        }

        // Apply reranking if needed
        if (relevantChunks.length > 10 &&
            (queryAnalysis.type === 'recommendation' || queryAnalysis.type === 'exploratory' || queryAnalysis.type === 'comparative')) {
            try {
                const rerankedChunks = await rerankChunks(question, relevantChunks, Math.min(relevantChunks.length, 50));
                relevantChunks = rerankedChunks.map(rc => ({
                    id: rc.id,
                    content: rc.content,
                    similarity: rc.combinedScore,
                    metadata: rc.metadata,
                    documentId: rc.documentId,
                    chunkIndex: rc.chunkIndex
                }));
            } catch (rerankError: any) {
                console.warn(`[ChatService Stream] Reranking failed: ${rerankError.message}`);
            }
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // STEP 3: Build context and prompts (same as non-streaming)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let context: string;
        if (queryAnalysis.type === 'aggregation' || queryAnalysis.type === 'exploratory') {
            const groupedChunks = groupChunksByDocument(relevantChunks);
            context = formatGroupedContext(groupedChunks);
        } else {
            context = relevantChunks
                .map((chunk, index) => {
                    const metadata = chunk.metadata || {};
                    return `[ID: ${index + 1} | Fonte: ${metadata.fileName || 'Documento'}]\n${chunk.content}`;
                })
                .join('\n\n---\n\n');
        }

        const aggregationPrompt = generateAggregationPrompt(question, queryAnalysis);

        // Build User Profile Context
        let userProfileContext = '';
        if (userProfile) {
            userProfileContext = `\nPERFIL: ${userProfile.name || 'UsuÃ¡rio'}, ${userProfile.jobTitle || ''}, ${userProfile.department || ''}\n`;
        }

        // Simplified system prompt for streaming (same structure but no need to repeat full prompts)
        let systemPrompt = '';
        const baseContext = `REGRAS: Use APENAS os documentos abaixo. NÃƒO invente informaÃ§Ãµes.\n${userProfileContext}${searchMetadata}${aggregationPrompt}${contextInstruction}\n\nDOCUMENTOS:\n${context}`;

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // IMAGE LINKING (STREAM): Prepare image appendix
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        const imageChunks = relevantChunks.filter(c => c.metadata?.isImage === true);
        let imageAppendix = '';

        if (imageChunks.length > 0) {
            console.log(`[ChatService Stream] ğŸ–¼ï¸ Found ${imageChunks.length} image chunks relevant to query`);

            // Deduplicate images by normalized filename
            const uniqueImages = new Set<string>();
            const images: Array<{ fileName: string, storedName: string }> = [];

            for (const chunk of imageChunks) {
                const fileName = chunk.metadata.fileName;
                // Normalize filename for deduplication (case-insensitive, decode URL encoding)
                const normalizedFileName = decodeURIComponent(fileName || '').toLowerCase().trim();

                if (fileName && !uniqueImages.has(normalizedFileName)) {
                    uniqueImages.add(normalizedFileName);

                    let storedName = chunk.metadata.storedFileName;

                    // Fallback: If storedFileName is missing (old documents), fetch from DB
                    if (!storedName && chunk.documentId) {
                        try {
                            const doc = await prisma.document.findUnique({
                                where: { id: chunk.documentId },
                                select: { filePath: true }
                            });
                            if (doc?.filePath) {
                                storedName = path.basename(doc.filePath);
                            }
                        } catch (err) {
                            console.warn(`[ChatService Stream] Failed to resolve file path for doc ${chunk.documentId}`);
                        }
                    }

                    images.push({
                        fileName,
                        storedName: storedName || fileName // Fallback to original name if all else fails
                    });
                }
            }

            if (images.length > 0) {
                imageAppendix = '\n\n---\n\n### Imagens Relacionadas:\n\n';
                images.forEach(img => {
                    const encodedName = encodeURIComponent(img.storedName);
                    const imageUrl = `http://localhost:3001/uploads/${encodedName}`;
                    imageAppendix += `![${img.fileName}](${imageUrl})\n\n`;
                });
                console.log(`[ChatService Stream] Added ${images.length} unique images to response`);
            }
        }



        const tableInstruction = isTableMode ? '\n\nUSE TABELAS MARKDOWN quando apropriado.' : '';

        // Mode-specific prompts (simplified)
        switch (mode) {
            case 'direct':
                systemPrompt = `Especialista tÃ©cnico Tecfag. Respostas objetivas e diretas.\n\n${baseContext}${tableInstruction}`;
                break;
            case 'casual':
                systemPrompt = `Colega experiente Tecfag. Tom informal e prestativo.\n\n${baseContext}${tableInstruction}`;
                break;
            case 'professional':
                systemPrompt = `Consultor de vendas Tecfag. Abordagem consultiva SPICED quando relevante.\n\n${baseContext}${tableInstruction}`;
                break;
            default:
                systemPrompt = `Especialista tÃ©cnico Tecfag. Tom educativo, explicando conceitos.\n\n${baseContext}${tableInstruction}`;
                break;
        }

        const userPrompt = `PERGUNTA: "${question}"\n\nResponda baseado nos documentos acima.`;

        // Build messages for Gemini
        const geminiMessages = [
            { role: 'user', parts: [{ text: systemPrompt }] },
            { role: 'model', parts: [{ text: `Entendido. Modo ${mode} ativado.` }] },
            ...chatHistory.slice(-6).map(msg => ({
                role: msg.role === 'user' ? 'user' : 'model',
                parts: [{ text: msg.content }]
            })),
            { role: 'user', parts: [{ text: userPrompt }] }
        ];

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // STEP 4: Stream response from Gemini
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let tokenUsage: ChatResponse['tokenUsage'] = undefined;
        let fullResponse = '';

        try {
            const model = genAI.getGenerativeModel({ model: GEMINI_MODEL });
            console.log(`[ChatService Stream] Starting Gemini streaming...`);

            const streamResult = await model.generateContentStream({
                contents: geminiMessages as any,
                generationConfig: {
                    temperature: 0.3,
                    maxOutputTokens: 12000
                }
            });

            for await (const chunk of streamResult.stream) {
                const chunkText = chunk.text();
                if (chunkText) {
                    fullResponse += chunkText;
                    yield { type: 'chunk', content: chunkText };
                }
            }

            // Append images at the end of stream
            if (imageAppendix) {
                yield { type: 'chunk', content: imageAppendix };
            }

            // Get final token usage
            const finalResponse = await streamResult.response;
            const usageMetadata = finalResponse.usageMetadata;
            if (usageMetadata) {
                tokenUsage = {
                    inputTokens: usageMetadata.promptTokenCount || 0,
                    outputTokens: usageMetadata.candidatesTokenCount || 0,
                    totalTokens: usageMetadata.totalTokenCount || 0,
                    model: GEMINI_MODEL,
                };
            }

        } catch (geminiError: any) {
            // Fallback to Groq streaming
            console.warn(`[ChatService Stream] Gemini error: ${geminiError.message}. Trying Groq fallback...`);

            try {
                const groqMessages: any[] = [
                    { role: 'system', content: systemPrompt },
                    ...chatHistory.slice(-4).map(msg => ({
                        role: msg.role === 'assistant' ? 'assistant' : 'user',
                        content: msg.content
                    })),
                    { role: 'user', content: userPrompt }
                ];

                const stream = await groq.chat.completions.create({
                    messages: groqMessages,
                    model: GROQ_MODEL,
                    temperature: 0.3,
                    max_tokens: 4096,
                    stream: true,
                });

                for await (const chunk of stream) {
                    const chunkText = chunk.choices[0]?.delta?.content || '';
                    if (chunkText) {
                        fullResponse += chunkText;
                        yield { type: 'chunk', content: chunkText };
                    }
                }

                // Add fallback indicator
                yield { type: 'chunk', content: '\n\n*(Backup: Groq Llama 3.3)*' };

            } catch (groqError: any) {
                console.error('[ChatService Stream] Both providers failed:', groqError);
                yield { type: 'error', content: `Erro: ${groqError.message}` };
                return;
            }
        }

        // Extract sources
        const sources = relevantChunks.map((chunk) => ({
            fileName: chunk.metadata?.fileName || 'Documento desconhecido',
            chunkIndex: chunk.chunkIndex,
            similarity: chunk.similarity
        }));

        console.log(`[ChatService Stream] âœ… Completed (${fullResponse.length} chars)`);
        yield { type: 'done', sources, tokenUsage };

    } catch (error: any) {
        console.error('[ChatService Stream] Error:', error);
        yield { type: 'error', content: error.message };
    }
}

/**
 * Generate suggested questions based on available documents
 */
export async function generateSuggestedQuestions(
    catalogId?: string,
    count: number = 3
): Promise<string[]> {
    try {
        // Reduced sample size for speed
        const sampleChunks = await searchSimilarChunks(
            Array(768).fill(0.1),
            8,
            catalogId ? { catalogId } : undefined
        );

        if (sampleChunks.length === 0) {
            return [
                'Quais documentos estÃ£o disponÃ­veis?',
                'O que este catÃ¡logo cobre?',
                'Como posso comeÃ§ar?'
            ];
        }

        const sampleText = sampleChunks
            .slice(0, 5)
            .map(c => `[doc] ${c.content.substring(0, 300)}`)
            .join('\n');

        const prompt = `Gere ${count} perguntas curtas e tÃ©cnicas (max 10 palavras) que um engenheiro faria sobre estes textos:
${sampleText}
Apenas as perguntas, uma por linha.`;

        let questionsText = "";

        // Use Gemini 2.5 Flash for suggestions (faster, simpler query)
        try {
            const model = genAI.getGenerativeModel({ model: GEMINI_MODEL });
            const result = await model.generateContent(prompt);
            questionsText = result.response.text();
        } catch (geminiError: any) {
            // Fallback to Groq
            console.warn('[ChatService] Gemini failed for suggestions, using Groq fallback');
            const completion = await groq.chat.completions.create({
                messages: [{ role: 'user', content: prompt }],
                model: GROQ_MODEL,
                temperature: 0.5,
            });
            questionsText = completion.choices[0]?.message?.content || "";
        }

        const questions = questionsText
            .split('\n')
            .map(q => q.trim())
            .filter(q => q.length > 5 && q.includes('?'))
            .slice(0, count);

        return questions.length > 0 ? questions : [
            'Quais os principais riscos?',
            'Como realizar a manutenÃ§Ã£o?',
            'Quais as especificaÃ§Ãµes tÃ©cnicas?'
        ];

    } catch (error) {
        console.error('[ChatService] Suggestion Error:', error);
        return [
            'Quais sÃ£o os pontos principais?',
            'Existem riscos operacionais?',
            'O que diz sobre manutenÃ§Ã£o?'
        ];
    }
}
