import { GoogleGenerativeAI } from '@google/generative-ai';
import { generateEmbedding } from './embeddings';
import { searchSimilarChunks, getDocumentStats } from './vectorDB';
import { analyzeQuery, generateAggregationPrompt, QueryAnalysis } from './queryAnalyzer';
import { multiQuerySearch, groupChunksByDocument, formatGroupedContext } from './multiQueryRAG';
import { rerankChunks } from './reranker';
import * as sessionMemory from './sessionMemory';
import * as cacheService from './cacheService';
import Groq from 'groq-sdk';
import notificationService from '../notificationService';
import { prisma } from '../../config/database';
import * as path from 'path';

// Gemini 2.5 Flash como provider principal, Groq como fallback
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
const groq = new Groq({ apiKey: process.env.GROQ_API_KEY || 'dummy' });

// Modelo principal: Gemini 2.5 Flash
const GEMINI_MODEL = 'gemini-2.5-flash';
const GROQ_MODEL = 'llama-3.3-70b-versatile';

export interface ChatMessage {
    role: 'user' | 'assistant';
    content: string;
}

export interface ChatResponse {
    response: string;
    sources: Array<{
        fileName: string;
        chunkIndex: number;
        similarity: number;
    }>;
    tokenUsage?: {
        inputTokens: number;
        outputTokens: number;
        totalTokens: number;
        model: string;
    };
    fromCache?: boolean;  // Indica se a resposta veio do cache
}

export interface UserProfile {
    userId?: string;  // Required for session context
    name?: string;
    jobTitle?: string;
    department?: string;
    technicalLevel?: string;
    communicationStyle?: string;
}

/**
 * Generate a greeting response without RAG (for simple greetings)
 */
function generateGreetingResponse(question: string, mode: string): string {
    const lowerQ = question.toLowerCase().trim();

    // Detect time-based greetings
    const hour = new Date().getHours();
    let timeGreeting = 'Ol√°';
    if (hour >= 5 && hour < 12) timeGreeting = 'Bom dia';
    else if (hour >= 12 && hour < 18) timeGreeting = 'Boa tarde';
    else timeGreeting = 'Boa noite';

    // Professional mode greeting
    if (mode === 'professional') {
        return `${timeGreeting}! Sou o assistente comercial da Tecfag. Como posso ajud√°-lo hoje com nossas solu√ß√µes de equipamentos e automa√ß√£o?`;
    }

    // Casual mode greeting
    if (mode === 'casual') {
        return `${timeGreeting}! üëã Tudo bem? Estou aqui para ajudar com qualquer d√∫vida sobre os produtos e solu√ß√µes da Tecfag. O que precisa?`;
    }

    // Direct mode greeting
    if (mode === 'direct') {
        return `${timeGreeting}. Como posso ajudar?`;
    }

    // Educational/default greeting
    return `${timeGreeting}! Sou o assistente t√©cnico da Tecfag. Estou aqui para ajudar com informa√ß√µes sobre nossos equipamentos, especifica√ß√µes t√©cnicas e orienta√ß√µes. Como posso ajud√°-lo hoje?`;
}

/**
 * Answer a question using RAG (Retrieval Augmented Generation)
 */
export async function answerQuestion(
    question: string,
    catalogId?: string,
    chatHistory: ChatMessage[] = [],
    mode: 'direct' | 'casual' | 'educational' | 'professional' = 'educational',
    isTableMode: boolean = false,
    userProfile?: UserProfile,
    isAttachmentMode: boolean = false
): Promise<ChatResponse> {
    try {
        console.log(`[ChatService] Processing question: ${question.substring(0, 50)}... (Mode: ${mode}, Provider: Gemini 2.5 Flash, Attachment: ${isAttachmentMode})`);

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // SESSION CONTEXT: Process user context for memory-aware responses
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let contextInstruction = '';
        if (userProfile?.userId) {
            try {
                const responseFormat = isTableMode ? 'table' : 'normal';
                const { contextInstruction: ctxInst } = await sessionMemory.processMessageContext(
                    userProfile.userId,
                    question,
                    responseFormat
                );
                contextInstruction = ctxInst;
                console.log(`[ChatService] Session context loaded for user ${userProfile.userId}`);
            } catch (ctxError: any) {
                console.warn(`[ChatService] Session context error (non-fatal): ${ctxError.message}`);
            }
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // ADVANCED RAG: Step 1 - Analyze the query to determine strategy
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        const queryAnalysis = analyzeQuery(question);

        console.log(`[ChatService] Query Analysis:`, {
            type: queryAnalysis.type,
            contextSize: queryAnalysis.contextSize,
            needsMultiQuery: queryAnalysis.needsMultiQuery,
            isCountQuery: queryAnalysis.isCountQuery,
            categories: queryAnalysis.categories
        });

        // Handle greetings without RAG
        if (queryAnalysis.type === 'greeting') {
            return {
                response: generateGreetingResponse(question, mode),
                sources: []
            };
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // CACHE CHECK: Step 1.5 - Check for cached response
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // First, check exact cache (by hash)
        const cachedExact = await cacheService.getCachedResponseExact(question, catalogId);
        if (cachedExact) {
            console.log(`[ChatService] üöÄ Returning CACHED response (exact match)`);
            await cacheService.recordCacheHit(cachedExact.id);
            return {
                response: cachedExact.response,
                sources: cachedExact.sources,
                fromCache: true
            };
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // ADVANCED RAG: Step 2 - Multi-query search or standard search
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let relevantChunks;
        let searchMetadata = '';

        if (queryAnalysis.needsMultiQuery) {
            // Use advanced multi-query RAG for aggregation/exploratory queries
            console.log(`[ChatService] Using Multi-Query RAG (${queryAnalysis.suggestedQueries.length + 1} queries)`);

            const multiResult = await multiQuerySearch(question, queryAnalysis, catalogId);
            relevantChunks = multiResult.chunks;

            // Add metadata about the search for better context
            searchMetadata = `
üìä INFORMA√á√ÉO DO SISTEMA (use para contexto):
- Foram consultados ${multiResult.uniqueDocuments.length} documentos diferentes
- Recuperados ${multiResult.chunks.length} trechos relevantes
- Queries executadas: ${multiResult.queryBreakdown.map(q => q.query).slice(0, 3).join(', ')}...
`;

            // For count queries, also get document stats
            if (queryAnalysis.isCountQuery) {
                const stats = await getDocumentStats(catalogId);
                searchMetadata += `
üìà ESTAT√çSTICAS DA BASE:
- Total de documentos indexados: ${stats.totalDocuments}
- Total de chunks na base: ${stats.totalChunks}
- Documentos: ${stats.documentNames.slice(0, 10).join(', ')}${stats.documentNames.length > 10 ? '...' : ''}
`;
            }
        } else {
            // Standard semantic search for factual queries
            // First check embedding cache
            let questionEmbedding = await cacheService.getEmbeddingFromCache(question);
            if (!questionEmbedding) {
                questionEmbedding = await generateEmbedding(question);
                await cacheService.cacheEmbedding(question, questionEmbedding);
            }

            // Check semantic cache before doing full RAG
            const cachedSemantic = await cacheService.getCachedResponseSemantic(questionEmbedding, catalogId);
            if (cachedSemantic) {
                console.log(`[ChatService] üöÄ Returning CACHED response (semantic match)`);
                await cacheService.recordCacheHit(cachedSemantic.id);
                return {
                    response: cachedSemantic.response,
                    sources: cachedSemantic.sources,
                    fromCache: true
                };
            }

            relevantChunks = await searchSimilarChunks(
                questionEmbedding,
                queryAnalysis.contextSize,
                catalogId ? { catalogId } : undefined
            );
        }

        console.log(`[ChatService] Found ${relevantChunks.length} relevant chunks`);

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // RERANKING: Use Gemini to reorder chunks by relevance
        // This is KEY for matching NotebookLM quality
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if (relevantChunks.length > 10 &&
            (queryAnalysis.type === 'recommendation' || queryAnalysis.type === 'exploratory' || queryAnalysis.type === 'comparative')) {
            console.log(`[ChatService] üéØ Applying reranking for ${queryAnalysis.type} query...`);
            try {
                const rerankedChunks = await rerankChunks(question, relevantChunks, Math.min(relevantChunks.length, 50));
                // Convert back to expected format
                relevantChunks = rerankedChunks.map(rc => ({
                    id: rc.id,
                    content: rc.content,
                    similarity: rc.combinedScore, // Use combined score
                    metadata: rc.metadata,
                    documentId: rc.documentId,
                    chunkIndex: rc.chunkIndex
                }));
                console.log(`[ChatService] ‚úÖ Reranking complete. Top chunk score: ${relevantChunks[0]?.similarity.toFixed(3)}`);
            } catch (rerankError: any) {
                console.warn(`[ChatService] ‚ö†Ô∏è Reranking failed, using original order: ${rerankError.message}`);
            }
        }

        // Log chunk distribution for debugging
        const chunksByDoc = new Map<string, number>();
        for (const chunk of relevantChunks) {
            const fileName = chunk.metadata?.fileName || 'Unknown';
            chunksByDoc.set(fileName, (chunksByDoc.get(fileName) || 0) + 1);
        }
        console.log(`[ChatService] Chunk distribution:`, Object.fromEntries(chunksByDoc));

        if (relevantChunks.length === 0) {
            return {
                response: 'N√£o encontrei informa√ß√µes suficientes nos documentos para responder sua pergunta com a profundidade necess√°ria. Tente adicionar mais documentos relacionados ou reformule a pergunta.',
                sources: []
            };
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // ADVANCED RAG: Step 3 - Build context (grouped by document for aggregation)
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let context: string;

        if (queryAnalysis.type === 'aggregation' || queryAnalysis.type === 'exploratory') {
            // Group chunks by document for better understanding
            const groupedChunks = groupChunksByDocument(relevantChunks);
            context = formatGroupedContext(groupedChunks);
        } else {
            // Standard context formatting for factual queries
            context = relevantChunks
                .map((chunk, index) => {
                    const metadata = chunk.metadata || {};
                    return `[ID: ${index + 1} | Fonte: ${metadata.fileName || 'Documento'}]
${chunk.content}`;
                })
                .join('\n\n---\n\n');
        }

        // Add aggregation-specific prompt if needed
        const aggregationPrompt = generateAggregationPrompt(question, queryAnalysis);

        // Add recommendation-specific prompt for multi-option responses
        const recommendationPrompt = queryAnalysis.type === 'recommendation' ? `
üìç **PERGUNTA DE RECOMENDA√á√ÉO DETECTADA**
Esta pergunta pede uma RECOMENDA√á√ÉO de equipamento/m√°quina. Siga estas regras OBRIGAT√ìRIAS:

1. **IDENTIFICA√á√ÉO DO PRODUTO** (SE N√ÉO ESPECIFICADO):
   - Se a pergunta n√£o especificar CLARAMENTE o tipo de produto (p√≥, l√≠quido, gr√£o, pastoso), o formato desejado (sach√™ stick, sach√™ 3 soldas, pouch, etc.), ou o volume de produ√ß√£o, PERGUNTE PRIMEIRO antes de recomendar.
   - Exemplo: "Para recomendar a m√°quina ideal, preciso saber: qual tipo de produto ser√° embalado (p√≥, gr√£os, l√≠quido)? Qual o formato de sach√™ desejado?"

2. **M√öLTIPLAS OP√á√ïES (OBRIGAT√ìRIO)**:
   - SEMPRE apresente pelo menos **3-4 op√ß√µes de m√°quinas diferentes**, organizadas por categoria/aplica√ß√£o.
   - Use formata√ß√£o estruturada com headers para cada categoria:
     ### 1. Para Sach√™s tipo [Tipo] (Aplica√ß√£o)
     - **Modelo**: [Nome da m√°quina]
     - **Dosagem**: [Faixa em g ou ml]
     - **Produtividade**: [embalagens/min]
     - **Tipo de Selagem**: [descri√ß√£o]
   
3. **COMPARA√á√ÉO PR√ÅTICA**:
   - Ao final, compare brevemente QUANDO cada op√ß√£o √© mais indicada.
   - Adicione uma "Dica de Especialista" BREVE (m√°ximo 2-3 linhas) - N√ÉO domine a resposta com metodologia SPICED.

4. **PRIORIZE ESPECIFICA√á√ïES T√âCNICAS**:
   - Foque em informa√ß√µes objetivas: capacidade, produtividade, tipo de selagem.
   - N√ÉO use metodologia SPICED para perguntas de recomenda√ß√£o t√©cnica.
   - Seja como um cat√°logo t√©cnico interativo, n√£o um vendedor.

303: EXEMPLO DE ESTRUTURA ESPERADA:
"Para recomendar a m√°quina ideal para embalar e envasar sach√™, identifiquei as seguintes op√ß√µes com base no tipo de produto:

### 1. Para Sach√™s tipo Stick (P√≥s)
- **Modelo**: AFPP2830B
- **Dosagem**: 10 a 30g
- **Produtividade**: 25 a 35 emb/min
- **Selagem**: 3 soldas

### 2. Para Sach√™s Tradicionais (P√≥s/Granulados)
[...]

### Qual escolher?
- Se o produto for p√≥ fino: opte pela AFPP2830B
- Se precisar de sach√™s com 4 soldas: AFPP1528A
[...]"` : '';

        // Build User Profile Context
        let userProfileContext = '';
        if (userProfile) {
            userProfileContext = `
PERFIL DO USU√ÅRIO (Personalize a resposta para esta pessoa):
- Nome: ${userProfile.name || 'Desconhecido'}
- Cargo: ${userProfile.jobTitle || 'N√£o informado'}
- Departamento: ${userProfile.department || 'N√£o informado'}
- N√≠vel T√©cnico: ${userProfile.technicalLevel || 'Padr√£o'}
- Estilo Preferido: ${userProfile.communicationStyle || 'Padr√£o'}

INSTRU√á√ÉO DE PERSONALIZA√á√ÉO:
- Adapte o vocabul√°rio e a profundidade t√©cnica ao N√≠vel T√©cnico do usu√°rio.
- D√™ exemplos relevantes ao Cargo e Departamento do usu√°rio.
- Se o estilo for "Visual", use muitas listas e tabelas.
- Se o estilo for "Direto", seja extremamente conciso.
- Responda como se estivesse falando diretamente para esta pessoa espec√≠fica.
`;
        }

        // 4. Build prompt based on Mode
        let systemPrompt = '';

        const baseContext = `
REGRAS DE FONTE (RAG) - LEIA COM ATEN√á√ÉO:

üìå REGRA PRINCIPAL - USO EXCLUSIVO DOS DOCUMENTOS:
- Baseie sua resposta ESTRITAMENTE nos documentos fornecidos abaixo.
- N√ÉO invente informa√ß√µes que n√£o estejam nos documentos.
- N√ÉO busque informa√ß√µes na internet, web, ou qualquer fonte externa.
- N√ÉO use seu conhecimento pr√©vio de treinamento para complementar respostas.
- Toda informa√ß√£o na sua resposta DEVE vir dos documentos anexados abaixo.

üìå REGRA CR√çTICA - USE TODOS OS DOCUMENTOS:
- Voc√™ tem acesso a TODOS os documentos relevantes para esta pergunta.
- NUNCA diga que informa√ß√µes n√£o estavam nos "documentos iniciais" - esse termo n√£o existe.
- NUNCA invente limita√ß√µes sobre quais documentos voc√™ tem acesso.
- Se a informa√ß√£o est√° em QUALQUER documento fornecido, voc√™ DEVE inclu√≠-la na resposta.
- Analise TODOS os trechos fornecidos antes de responder.

üìå REGRA DE TRANSPAR√äNCIA - QUANDO INFORMA√á√ÉO N√ÉO EXISTE:
- Se ap√≥s analisar TODOS os documentos fornecidos voc√™ n√£o encontrar a informa√ß√£o solicitada, diga claramente:
  "N√£o encontrei informa√ß√µes sobre [tema] nos documentos cadastrados no sistema. Pode ser que esse conte√∫do ainda n√£o tenha sido adicionado √† base de conhecimento."
- Seja espec√≠fico sobre O QUE n√£o foi encontrado, n√£o generalize.
- NUNCA use a desculpa de "documentos iniciais" ou "primeiros documentos".

üìå CITA√á√ÉO DE FONTES:
- N√ÉO cite as fontes no texto da resposta (ex: "Segundo documento X").
- As fontes ser√£o apresentadas separadamente pela interface do sistema.

${userProfileContext}
${searchMetadata}
${aggregationPrompt}
${recommendationPrompt}
${contextInstruction}

DOCUMENTOS DE REFER√äNCIA (USE TODO O CONTE√öDO ABAIXO):
${context}`;

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // IMAGE LINKING: Find relevant images in chunks and append to response
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let imageAppendix = '';

        // Only process images if attachment mode is active
        if (isAttachmentMode) {
            const imageChunks = relevantChunks.filter(c => c.metadata?.isImage === true);

            if (imageChunks.length > 0) {
                console.log(`[ChatService] üñºÔ∏è Found ${imageChunks.length} image chunks relevant to query`);

                // Deduplicate images by normalized filename (case-insensitive, URL-decoded)
                const uniqueImages = new Set<string>();
                const images: Array<{ fileName: string, storedName: string }> = [];

                for (const chunk of imageChunks) {
                    const fileName = chunk.metadata.fileName;
                    // Normalize filename for deduplication
                    const normalizedFileName = decodeURIComponent(fileName || '').toLowerCase().trim();

                    if (fileName && !uniqueImages.has(normalizedFileName)) {
                        uniqueImages.add(normalizedFileName);

                        let storedName = chunk.metadata.storedFileName;

                        // Fallback: If storedFileName is missing (old documents), fetch from DB
                        if (!storedName && chunk.documentId) {
                            try {
                                const doc = await prisma.document.findUnique({
                                    where: { id: chunk.documentId },
                                    select: { filePath: true }
                                });
                                if (doc?.filePath) {
                                    storedName = path.basename(doc.filePath);
                                }
                            } catch (err) {
                                console.warn(`[ChatService] Failed to resolve file path for doc ${chunk.documentId}`);
                            }
                        }

                        images.push({
                            fileName,
                            storedName: storedName || fileName // Fallback to original name if all else fails
                        });
                    }
                }

                if (images.length > 0) {
                    imageAppendix = '\n\n---\n\n### Imagens Relacionadas:\n\n';
                    images.forEach(img => {
                        // Encode filename for URL
                        const encodedName = encodeURIComponent(img.storedName);
                        // Use absolute URL to backend static files
                        // Assuming uploadDir is mounted at /uploads
                        const imageUrl = `http://localhost:3001/uploads/${encodedName}`;
                        imageAppendix += `![${img.fileName}](${imageUrl})\n\n`;
                    });
                    console.log(`[ChatService] Added ${images.length} images to response`);
                }
            }
        }

        const tableInstruction = isTableMode
            ? `\n\nREQUISITO ESPECIAL DE FORMATA√á√ÉO:
- O usu√°rio ATIVOU o "Modo Tabela".
- Voc√™ DEVE apresentar a resposta ou parte significativa dela em formato de TABELA MARKDOWN sempre que houver dados compar√°veis ou list√°veis.
- Se a pergunta for sobre compara√ß√£o, diferen√ßas, especifica√ß√µes ou listas, a tabela √© OBRIGAT√ìRIA.
- Use colunas claras e objetivas.`
            : '';

        switch (mode) {
            case 'direct':
                systemPrompt = `Voc√™ √© um especialista t√©cnico da Tecfag que valoriza o tempo do colega.

Responda de forma objetiva e eficiente. Se for sim ou n√£o, comece assim.
Quando listar informa√ß√µes, fa√ßa de forma organizada, mas sem perder naturalidade.
N√£o use introdu√ß√µes desnecess√°rias - v√° direto ao que importa.

${baseContext}
${tableInstruction}`;
                break;

            case 'casual':
                systemPrompt = `Voc√™ √© um colega experiente da Tecfag batendo um papo.

Responda como se estivesse conversando no corredor ou tomando um caf√©.
Seja natural - pode usar express√µes do dia a dia, mas sem exagerar.
Valide d√∫vidas quando fizer sentido ("Boa pergunta", "√â, isso confunde mesmo").
Seja prestativo sem ser formal.

${baseContext}
${tableInstruction}`;
                break;

            case 'professional':
                systemPrompt = `CONTEXTO: Voc√™ √© um CONSULTOR DE VENDAS ESPECIALISTA da Tecfag Group.

PAPEL E IDENTIDADE:
- Voc√™ √© um especialista comercial da Tecfag Group com profundo conhecimento em solu√ß√µes t√©cnicas, processos industriais e automa√ß√£o.
- Voc√™ fala como um consultor experiente conversando com um colega, N√ÉO como um rob√¥ ou chatbot.
- Seu objetivo √© ENSINAR o vendedor a vender de forma consultiva, n√£o apenas listar informa√ß√µes.

DETEC√á√ÉO DE CONTEXTO E PROPOR√á√ÉO DE RESPOSTA (CR√çTICO):
Antes de responder, AVALIE a complexidade e o tipo da pergunta:

üìç **SAUDA√á√ïES E MENSAGENS SOCIAIS** (ex: "bom dia", "ol√°", "como vai?"):
- Responda de forma CORDIAL e BREVE
- N√ÉO aplique SPICED
- N√ÉO inclua Dica de Especialista
- N√ÉO liste produtos ou solu√ß√µes n√£o solicitados
- Exemplo: "Bom dia! Como posso ajud√°-lo hoje com as solu√ß√µes da Tecfag?"

üìç **PERGUNTAS FACTUAIS SIMPLES** (ex: "Qual o pre√ßo?", "Onde fica a empresa?"):
- Responda DIRETAMENTE com a informa√ß√£o solicitada
- N√ÉO aplique SPICED
- Seja objetivo e profissional

üìç **PERGUNTAS SOBRE VENDAS/CONSULTORIA** (ex: "Como vender X?", "Como usar t√©cnica Y?"):
- APLIQUE SPICED de forma narrativa e fluida
- INCLUA Dica de Especialista com analogia memor√°vel
- Use estrutura consultiva completa

üìç **PERGUNTAS T√âCNICAS COMPLEXAS** (ex: "Como funciona X?", "Comparar A vs B"):
- Use abordagem consultiva com dados t√©cnicos integrados
- SPICED pode ser aplicado se agregar valor ao argumento de vendas
- Dica de Especialista OPCIONAL, apenas se genuinamente √∫til

METODOLOGIA DE VENDAS (SPICED - Uso Condicional):
Quando a pergunta for sobre VENDAS, CONSULTORIA ou PRODUTOS, estruture a resposta usando SPICED de forma NARRATIVA e FLUIDA:
- Situation: Explique como entender o contexto do cliente
- Pain: Identifique as dores espec√≠ficas que o produto resolve
- Impact: Quantifique o valor e ROI da solu√ß√£o
- Critical Event: Identifique gatilhos de urg√™ncia
- Decision: Facilite o processo de decis√£o

ESTILO DE RESPOSTA NARRATIVO:
‚úÖ **FA√áA:**
- Escreva como um especialista explicando para outro profissional (narrativa fluida, n√£o listas mec√¢nicas)
- Para cada etapa do SPICED, inclua uma **"Pergunta chave:"** espec√≠fica e pr√°tica que o vendedor pode usar
- Integre dados t√©cnicos NATURALMENTE no argumento de vendas (n√£o como lista separada)
- Use marcadores (‚Ä¢) apenas para destacar pontos-chave dentro da narrativa
- Quando usar SPICED completo, inclua uma se√ß√£o **"Dica de Especialista:"** com uma ANALOGIA MEMOR√ÅVEL

‚ùå **EVITE:**
- Aplicar estruturas complexas em perguntas simples
- Listas gen√©ricas sem contexto
- Tom rob√≥tico ou formato de checklist
- Separar "Benef√≠cios" do texto principal (integre no argumento)
- Perguntas vagas - seja ESPEC√çFICO com dados do produto

ESTRUTURA ESPERADA (para perguntas de vendas/consultoria):
1. **Introdu√ß√£o consultiva** explicando a abordagem
2. **Desenvolvimento narrativo** para cada etapa do SPICED:
   - Explica√ß√£o do objetivo da etapa
   - ‚Ä¢ **Aplica√ß√£o**: Como aplicar com o produto espec√≠fico
   - ‚Ä¢ **Pergunta chave**: "[pergunta espec√≠fica que o vendedor pode fazer]"
   - Destaque dados t√©cnicos integrados naturalmente
3. **Dica de Especialista**: Inclua analogia poderosa e memor√°vel que compare o produto/processo atual a algo familiar
4. **Conclus√£o persuasiva** (opcional, se fizer sentido)

EXEMPLO DE TOM NARRATIVO:
‚úÖ "**1. Situa√ß√£o (Situation)** - O objetivo aqui √© entender o contexto atual do cliente. Pergunte sobre o volume de produ√ß√£o e os materiais utilizados. ‚Ä¢ **Aplica√ß√£o**: Verifique se o cliente trabalha com embalagens flex√≠veis como PP, PE, BOPP. ‚Ä¢ **Pergunta chave**: 'Como √© o seu processo de selagem hoje e qual o tamanho da sua produ√ß√£o atual?'. Saiba que a TC20 √© ideal para pequena escala, mas com opera√ß√£o cont√≠nua."

EXEMPLO DE ANALOGIA MEMOR√ÅVEL:
‚úÖ "**Dica de Especialista:** Para facilitar o entendimento do cliente sobre a versatilidade da m√°quina, use esta analogia: 'Imagine que sua produ√ß√£o hoje √© como lavar lou√ßa √† m√£o; voc√™ gasta tempo e esfor√ßo em cada pe√ßa individualmente. A Pratic Seal TC20 funciona como uma lavadora de lou√ßas: voc√™ apenas posiciona as embalagens na entrada e ela faz o trabalho de forma cont√≠nua e padronizada, permitindo que voc√™ foque em expandir seu neg√≥cio enquanto ela garante o fechamento perfeito.'"

INTEGRA√á√ÉO DE DADOS T√âCNICOS:
- N√ÉO crie listas separadas de especifica√ß√µes (exceto se solicitado ou em modo tabela)
- INTEGRE os dados t√©cnicos nos argumentos de forma natural
- Use os dados para QUANTIFICAR impacto e ROI

${baseContext}
${tableInstruction}

LEMBRE-SE: Seja PROPORCIONAL √† pergunta. Sauda√ß√µes merecem sauda√ß√µes. Perguntas complexas merecem respostas completas. Sua resposta deve parecer que foi escrita por um consultor HUMANO experiente que adapta sua comunica√ß√£o ao contexto.`;
                break;
            default:
                systemPrompt = `Voc√™ √© um especialista t√©cnico da Tecfag explicando para um colega.

Sua paix√£o √© ensinar e fazer as pessoas entenderem de verdade.
Explique o racioc√≠nio por tr√°s das coisas, n√£o apenas os fatos.
Use analogias quando ajudarem a clarear conceitos complexos.
Antecipe perguntas que a pessoa possa ter e responda-as naturalmente.

${baseContext}
${tableInstruction}`;
                break;
        }

        const userPrompt = `PERGUNTA DO USU√ÅRIO: "${question}"

Elabore uma resposta completa baseada nos documentos acima.`;

        // 5. Generate response - Gemini 2.5 Flash primary, Groq fallback
        let response: string = "";
        let tokenUsage: ChatResponse['tokenUsage'] = undefined;
        let usedFallback = false;

        // Build messages for Gemini
        const geminiMessages = [
            { role: 'user', parts: [{ text: systemPrompt }] },
            { role: 'model', parts: [{ text: `Entendido. Modo ${mode} ativado.` }] },
            ...chatHistory.slice(-6).map(msg => ({
                role: msg.role === 'user' ? 'user' : 'model',
                parts: [{ text: msg.content }]
            })),
            { role: 'user', parts: [{ text: userPrompt }] }
        ];

        try {
            // PRIMARY: Gemini 2.5 Pro (User Requested)
            const model = genAI.getGenerativeModel({ model: 'gemini-2.5-pro' });

            console.log(`[ChatService] Requesting completion from Gemini 2.5 Pro (High Reasoning)...`);
            const result = await model.generateContent({
                contents: geminiMessages as any,
                generationConfig: {
                    temperature: 0.2, // Low temp for analytical precision (NotebookLM style)
                    maxOutputTokens: 65536 // Massive output limit for full lists
                }
            });

            response = result.response.text();

            // Append images if found
            if (imageAppendix) {
                response += imageAppendix;
            }

            // Capture token usage from Gemini
            const usageMetadata = result.response.usageMetadata;
            if (usageMetadata) {
                tokenUsage = {
                    inputTokens: usageMetadata.promptTokenCount || 0,
                    outputTokens: usageMetadata.candidatesTokenCount || 0,
                    totalTokens: usageMetadata.totalTokenCount || 0,
                    model: GEMINI_MODEL,
                };
                console.log(`[ChatService] ‚úÖ Gemini 2.5 Flash - Token usage: ${tokenUsage.totalTokens} total (${tokenUsage.inputTokens} in, ${tokenUsage.outputTokens} out)`);
            }

        } catch (geminiError: any) {
            // FALLBACK: Groq (Llama 3.3 70B)
            console.warn(`[ChatService] ‚ö†Ô∏è Gemini error: ${geminiError.message}. Switching to Groq fallback...`);
            usedFallback = true;

            try {
                const groqMessages: any[] = [
                    { role: 'system', content: systemPrompt },
                    ...chatHistory.slice(-4).map(msg => ({
                        role: msg.role === 'assistant' ? 'assistant' : 'user',
                        content: msg.content
                    })),
                    { role: 'user', content: userPrompt }
                ];

                console.log('[ChatService] Requesting completion from Groq (Llama 3.3 70B) as fallback...');

                const completion = await groq.chat.completions.create({
                    messages: groqMessages,
                    model: GROQ_MODEL,
                    temperature: 0.3,
                    max_tokens: 4096,
                    top_p: 0.9,
                });

                response = completion.choices[0]?.message?.content || "";
                response += '\n\n*(Backup: Groq Llama 3.3)*';

                // Append images if found
                if (imageAppendix) {
                    response += imageAppendix;
                }

                // Capture token usage from Groq
                if (completion.usage) {
                    tokenUsage = {
                        inputTokens: completion.usage.prompt_tokens || 0,
                        outputTokens: completion.usage.completion_tokens || 0,
                        totalTokens: completion.usage.total_tokens || 0,
                        model: GROQ_MODEL + ' (fallback)',
                    };
                    console.log(`[ChatService] ‚úÖ Groq Fallback - Token usage: ${tokenUsage.totalTokens} total`);
                }

            } catch (groqError: any) {
                console.error('[ChatService] ‚ùå Both Gemini and Groq failed:', groqError);

                // Notificar admins sobre falha cr√≠tica de IA
                await notificationService.broadcastToAdmins(
                    'system',
                    'üö® Falha Cr√≠tica: APIs de IA Indispon√≠veis',
                    `Gemini: ${geminiError.message} | Groq: ${groqError.message}`,
                    'error',
                    { geminiError: geminiError.message, groqError: groqError.message }
                );

                throw new Error(`AI providers unavailable: Gemini (${geminiError.message}), Groq (${groqError.message})`);
            }
        }

        console.log(`[ChatService] ‚úÖ Generated response (${response.length} chars)`);

        // 6. Extract sources
        const sources = relevantChunks.map((chunk, index) => ({
            fileName: chunk.metadata?.fileName || 'Documento desconhecido',
            chunkIndex: chunk.chunkIndex,
            similarity: chunk.similarity
        }));

        // 7. Cache the response for future use
        try {
            const documentIds = [...new Set(relevantChunks.map(c => c.documentId).filter(Boolean))];
            const questionEmbedding = await cacheService.getEmbeddingFromCache(question) || await generateEmbedding(question);
            await cacheService.cacheResponse(
                question,
                questionEmbedding,
                response,
                sources,
                documentIds,
                catalogId
            );
        } catch (cacheError: any) {
            console.warn(`[ChatService] Failed to cache response (non-fatal): ${cacheError.message}`);
        }

        return {
            response,
            sources,
            tokenUsage,
        };

    } catch (error: any) {
        console.error('[ChatService] Error:', error);
        throw new Error(`Failed to generate response: ${error.message}`);
    }
}

/**
 * Streaming version of answerQuestion - yields text chunks as they arrive
 */
export async function* answerQuestionStream(
    question: string,
    catalogId?: string,
    chatHistory: ChatMessage[] = [],
    mode: 'direct' | 'casual' | 'educational' | 'professional' = 'educational',
    isTableMode: boolean = false,
    userProfile?: UserProfile
): AsyncGenerator<{ type: 'chunk' | 'done' | 'error'; content?: string; sources?: any[]; tokenUsage?: ChatResponse['tokenUsage'] }> {
    try {
        console.log(`[ChatService Stream] Processing question: ${question.substring(0, 50)}...`);

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // SESSION CONTEXT: Process user context for memory-aware responses
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let contextInstruction = '';
        if (userProfile?.userId) {
            try {
                const responseFormat = isTableMode ? 'table' : 'normal';
                const { contextInstruction: ctxInst } = await sessionMemory.processMessageContext(
                    userProfile.userId,
                    question,
                    responseFormat
                );
                contextInstruction = ctxInst;
                console.log(`[ChatService Stream] Session context loaded for user ${userProfile.userId}`);
            } catch (ctxError: any) {
                console.warn(`[ChatService Stream] Session context error (non-fatal): ${ctxError.message}`);
            }
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // STEP 1: Analyze query (same as non-streaming)
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        const queryAnalysis = analyzeQuery(question);

        // Handle greetings without RAG
        if (queryAnalysis.type === 'greeting') {
            const greetingResponse = generateGreetingResponse(question, mode);
            yield { type: 'chunk', content: greetingResponse };
            yield { type: 'done', sources: [] };
            return;
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // STEP 2: Retrieve relevant chunks (same as non-streaming)
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let relevantChunks;
        let searchMetadata = '';

        if (queryAnalysis.needsMultiQuery) {
            const multiResult = await multiQuerySearch(question, queryAnalysis, catalogId);
            relevantChunks = multiResult.chunks;
            searchMetadata = `\nüìä INFORMA√á√ÉO DO SISTEMA: Consultados ${multiResult.uniqueDocuments.length} documentos, ${multiResult.chunks.length} trechos.\n`;

            if (queryAnalysis.isCountQuery) {
                const stats = await getDocumentStats(catalogId);
                searchMetadata += `üìà ESTAT√çSTICAS: ${stats.totalDocuments} docs, ${stats.totalChunks} chunks.\n`;
            }
        } else {
            const questionEmbedding = await generateEmbedding(question);
            relevantChunks = await searchSimilarChunks(
                questionEmbedding,
                queryAnalysis.contextSize,
                catalogId ? { catalogId } : undefined
            );
        }

        if (relevantChunks.length === 0) {
            yield { type: 'chunk', content: 'N√£o encontrei informa√ß√µes suficientes nos documentos para responder sua pergunta.' };
            yield { type: 'done', sources: [] };
            return;
        }

        // Apply reranking if needed
        if (relevantChunks.length > 10 &&
            (queryAnalysis.type === 'recommendation' || queryAnalysis.type === 'exploratory' || queryAnalysis.type === 'comparative')) {
            try {
                const rerankedChunks = await rerankChunks(question, relevantChunks, Math.min(relevantChunks.length, 50));
                relevantChunks = rerankedChunks.map(rc => ({
                    id: rc.id,
                    content: rc.content,
                    similarity: rc.combinedScore,
                    metadata: rc.metadata,
                    documentId: rc.documentId,
                    chunkIndex: rc.chunkIndex
                }));
            } catch (rerankError: any) {
                console.warn(`[ChatService Stream] Reranking failed: ${rerankError.message}`);
            }
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // STEP 3: Build context and prompts (same as non-streaming)
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let context: string;
        if (queryAnalysis.type === 'aggregation' || queryAnalysis.type === 'exploratory') {
            const groupedChunks = groupChunksByDocument(relevantChunks);
            context = formatGroupedContext(groupedChunks);
        } else {
            context = relevantChunks
                .map((chunk, index) => {
                    const metadata = chunk.metadata || {};
                    return `[ID: ${index + 1} | Fonte: ${metadata.fileName || 'Documento'}]\n${chunk.content}`;
                })
                .join('\n\n---\n\n');
        }

        const aggregationPrompt = generateAggregationPrompt(question, queryAnalysis);

        // Build User Profile Context
        let userProfileContext = '';
        if (userProfile) {
            userProfileContext = `\nPERFIL: ${userProfile.name || 'Usu√°rio'}, ${userProfile.jobTitle || ''}, ${userProfile.department || ''}\n`;
        }

        // Simplified system prompt for streaming (same structure but no need to repeat full prompts)
        let systemPrompt = '';
        const baseContext = `REGRAS: Use APENAS os documentos abaixo. N√ÉO invente informa√ß√µes.\n${userProfileContext}${searchMetadata}${aggregationPrompt}${contextInstruction}\n\nDOCUMENTOS:\n${context}`;

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // IMAGE LINKING (STREAM): Prepare image appendix
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        const imageChunks = relevantChunks.filter(c => c.metadata?.isImage === true);
        let imageAppendix = '';

        if (imageChunks.length > 0) {
            console.log(`[ChatService Stream] üñºÔ∏è Found ${imageChunks.length} image chunks relevant to query`);

            // Deduplicate images by normalized filename
            const uniqueImages = new Set<string>();
            const images: Array<{ fileName: string, storedName: string }> = [];

            for (const chunk of imageChunks) {
                const fileName = chunk.metadata.fileName;
                // Normalize filename for deduplication (case-insensitive, decode URL encoding)
                const normalizedFileName = decodeURIComponent(fileName || '').toLowerCase().trim();

                if (fileName && !uniqueImages.has(normalizedFileName)) {
                    uniqueImages.add(normalizedFileName);

                    let storedName = chunk.metadata.storedFileName;

                    // Fallback: If storedFileName is missing (old documents), fetch from DB
                    if (!storedName && chunk.documentId) {
                        try {
                            const doc = await prisma.document.findUnique({
                                where: { id: chunk.documentId },
                                select: { filePath: true }
                            });
                            if (doc?.filePath) {
                                storedName = path.basename(doc.filePath);
                            }
                        } catch (err) {
                            console.warn(`[ChatService Stream] Failed to resolve file path for doc ${chunk.documentId}`);
                        }
                    }

                    images.push({
                        fileName,
                        storedName: storedName || fileName // Fallback to original name if all else fails
                    });
                }
            }

            if (images.length > 0) {
                imageAppendix = '\n\n---\n\n### Imagens Relacionadas:\n\n';
                images.forEach(img => {
                    const encodedName = encodeURIComponent(img.storedName);
                    const imageUrl = `http://localhost:3001/uploads/${encodedName}`;
                    imageAppendix += `![${img.fileName}](${imageUrl})\n\n`;
                });
                console.log(`[ChatService Stream] Added ${images.length} unique images to response`);
            }
        }



        const tableInstruction = isTableMode ? '\n\nUSE TABELAS MARKDOWN quando apropriado.' : '';

        // Mode-specific prompts (simplified)
        switch (mode) {
            case 'direct':
                systemPrompt = `Especialista t√©cnico Tecfag. Respostas objetivas e diretas.\n\n${baseContext}${tableInstruction}`;
                break;
            case 'casual':
                systemPrompt = `Colega experiente Tecfag. Tom informal e prestativo.\n\n${baseContext}${tableInstruction}`;
                break;
            case 'professional':
                systemPrompt = `Consultor de vendas Tecfag. Abordagem consultiva SPICED quando relevante.\n\n${baseContext}${tableInstruction}`;
                break;
            default:
                systemPrompt = `Especialista t√©cnico Tecfag. Tom educativo, explicando conceitos.\n\n${baseContext}${tableInstruction}`;
                break;
        }

        const userPrompt = `PERGUNTA: "${question}"\n\nResponda baseado nos documentos acima.`;

        // Build messages for Gemini
        const geminiMessages = [
            { role: 'user', parts: [{ text: systemPrompt }] },
            { role: 'model', parts: [{ text: `Entendido. Modo ${mode} ativado.` }] },
            ...chatHistory.slice(-6).map(msg => ({
                role: msg.role === 'user' ? 'user' : 'model',
                parts: [{ text: msg.content }]
            })),
            { role: 'user', parts: [{ text: userPrompt }] }
        ];

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // STEP 4: Stream response from Gemini
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        let tokenUsage: ChatResponse['tokenUsage'] = undefined;
        let fullResponse = '';

        try {
            const model = genAI.getGenerativeModel({ model: GEMINI_MODEL });
            console.log(`[ChatService Stream] Starting Gemini streaming...`);

            const streamResult = await model.generateContentStream({
                contents: geminiMessages as any,
                generationConfig: {
                    temperature: 0.3,
                    maxOutputTokens: 12000
                }
            });

            for await (const chunk of streamResult.stream) {
                const chunkText = chunk.text();
                if (chunkText) {
                    fullResponse += chunkText;
                    yield { type: 'chunk', content: chunkText };
                }
            }

            // Append images at the end of stream
            if (imageAppendix) {
                yield { type: 'chunk', content: imageAppendix };
            }

            // Get final token usage
            const finalResponse = await streamResult.response;
            const usageMetadata = finalResponse.usageMetadata;
            if (usageMetadata) {
                tokenUsage = {
                    inputTokens: usageMetadata.promptTokenCount || 0,
                    outputTokens: usageMetadata.candidatesTokenCount || 0,
                    totalTokens: usageMetadata.totalTokenCount || 0,
                    model: GEMINI_MODEL,
                };
            }

        } catch (geminiError: any) {
            // Fallback to Groq streaming
            console.warn(`[ChatService Stream] Gemini error: ${geminiError.message}. Trying Groq fallback...`);

            try {
                const groqMessages: any[] = [
                    { role: 'system', content: systemPrompt },
                    ...chatHistory.slice(-4).map(msg => ({
                        role: msg.role === 'assistant' ? 'assistant' : 'user',
                        content: msg.content
                    })),
                    { role: 'user', content: userPrompt }
                ];

                const stream = await groq.chat.completions.create({
                    messages: groqMessages,
                    model: GROQ_MODEL,
                    temperature: 0.3,
                    max_tokens: 4096,
                    stream: true,
                });

                for await (const chunk of stream) {
                    const chunkText = chunk.choices[0]?.delta?.content || '';
                    if (chunkText) {
                        fullResponse += chunkText;
                        yield { type: 'chunk', content: chunkText };
                    }
                }

                // Add fallback indicator
                yield { type: 'chunk', content: '\n\n*(Backup: Groq Llama 3.3)*' };

            } catch (groqError: any) {
                console.error('[ChatService Stream] Both providers failed:', groqError);
                yield { type: 'error', content: `Erro: ${groqError.message}` };
                return;
            }
        }

        // Extract sources
        const sources = relevantChunks.map((chunk) => ({
            fileName: chunk.metadata?.fileName || 'Documento desconhecido',
            chunkIndex: chunk.chunkIndex,
            similarity: chunk.similarity
        }));

        console.log(`[ChatService Stream] ‚úÖ Completed (${fullResponse.length} chars)`);
        yield { type: 'done', sources, tokenUsage };

    } catch (error: any) {
        console.error('[ChatService Stream] Error:', error);
        yield { type: 'error', content: error.message };
    }
}

/**
 * Generate suggested questions based on available documents
 */
export async function generateSuggestedQuestions(
    catalogId?: string,
    count: number = 3
): Promise<string[]> {
    try {
        // Reduced sample size for speed
        const sampleChunks = await searchSimilarChunks(
            Array(768).fill(0.1),
            8,
            catalogId ? { catalogId } : undefined
        );

        if (sampleChunks.length === 0) {
            return [
                'Quais documentos est√£o dispon√≠veis?',
                'O que este cat√°logo cobre?',
                'Como posso come√ßar?'
            ];
        }

        const sampleText = sampleChunks
            .slice(0, 5)
            .map(c => `[doc] ${c.content.substring(0, 300)}`)
            .join('\n');

        const prompt = `Gere ${count} perguntas curtas e t√©cnicas (max 10 palavras) que um engenheiro faria sobre estes textos:
${sampleText}
Apenas as perguntas, uma por linha.`;

        let questionsText = "";

        // Use Gemini 2.5 Flash for suggestions (faster, simpler query)
        try {
            const model = genAI.getGenerativeModel({ model: GEMINI_MODEL });
            const result = await model.generateContent(prompt);
            questionsText = result.response.text();
        } catch (geminiError: any) {
            // Fallback to Groq
            console.warn('[ChatService] Gemini failed for suggestions, using Groq fallback');
            const completion = await groq.chat.completions.create({
                messages: [{ role: 'user', content: prompt }],
                model: GROQ_MODEL,
                temperature: 0.5,
            });
            questionsText = completion.choices[0]?.message?.content || "";
        }

        const questions = questionsText
            .split('\n')
            .map(q => q.trim())
            .filter(q => q.length > 5 && q.includes('?'))
            .slice(0, count);

        return questions.length > 0 ? questions : [
            'Quais os principais riscos?',
            'Como realizar a manuten√ß√£o?',
            'Quais as especifica√ß√µes t√©cnicas?'
        ];

    } catch (error) {
        console.error('[ChatService] Suggestion Error:', error);
        return [
            'Quais s√£o os pontos principais?',
            'Existem riscos operacionais?',
            'O que diz sobre manuten√ß√£o?'
        ];
    }
}
